\frametitle{Demo: Forward, loss \& huấn luyện}
\textbf{Forward pass (mỗi timestep)}
    \[
        h_t = \tanh\!\left(x_t W_{xh} + h_{t-1} W_{hh} + b_h\right), \qquad
        o_t = h_t W_{qh} + b_q.
    \]
    \begin{itemize}
        \item Hidden activation: \(\tanh\).
        \item Output activation: tuyến tính (không activation).
    \end{itemize}
    \medskip
    \textbf{Loss function}
    \begin{itemize}
        \item Mean Squared Error (MSE):
    \end{itemize}
    \[
        \mathcal{L} = \frac{1}{2}\left(o_T - y\right)^2.
    \]
    \medskip
    \textbf{Huấn luyện}
    \begin{itemize}
        \item Backpropagation Through Time (BPTT).
        \item Gradient clipping để tránh exploding gradient:
        \[
            g \leftarrow \frac{g}{\max\!\left(1, \|g\| / \tau\right)}.
        \]
        \item Cập nhật tham số bằng Gradient Descent.
    \end{itemize}
    \vspace{0.4em}
    \footnotesize
\begin{verbatim}
h[t+1] = np.tanh(x[t] @ W_xh + h[t] @ W_hh + b_h)
o[t]   = h[t+1] @ W_qh + b_q

grads = model.backward(y_true=target, y_pred=outputs)
clipped = clip_gradients(grads, tau=tau)[0]
apply_gradients(model.parameters(), clipped, lr)
\end{verbatim}
