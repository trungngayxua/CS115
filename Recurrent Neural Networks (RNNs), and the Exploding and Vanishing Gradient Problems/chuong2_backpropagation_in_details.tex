\documentclass{beamer}

\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[vietnamese]{babel}
\usepackage{amsmath, amssymb, mathtools}
\usepackage{bm}
\usepackage{graphicx}

\title{Backpropagation Through Time in Detail}
\author{Nhóm trình bày}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}{Mục tiêu}
    \begin{itemize}
        \item Trình bày Backpropagation Through Time (BPTT) trên RNN tuyến tính, tập trung vào công thức gradient.
        \item Làm rõ cách tính $\displaystyle \frac{\partial \mathcal{L}}{\partial \bm{W}_{qh}}, \frac{\partial \mathcal{L}}{\partial \bm{W}_{hx}}, \frac{\partial \mathcal{L}}{\partial \bm{W}_{hh}}$ cùng các bias liên quan.
        \item Nhấn mạnh ý nghĩa của từng gradient trong cập nhật tham số mô hình.
    \end{itemize}
\end{frame}

\begin{frame}{Mô hình RNN tuyến tính}
    \begin{itemize}
        \item Chuỗi đầu vào $\bm{x}_t \in \mathbb{R}^d$, hidden state $\bm{h}_t \in \mathbb{R}^h$, đầu ra $\bm{o}_t \in \mathbb{R}^q$.
        \item Tham số: $\bm{W}_{hx} \in \mathbb{R}^{h \times d}$, $\bm{W}_{hh} \in \mathbb{R}^{h \times h}$, $\bm{W}_{qh} \in \mathbb{R}^{q \times h}$.
        \item Bias: $\bm{b}_h \in \mathbb{R}^h$ cho hidden layer, $\bm{b}_q \in \mathbb{R}^q$ cho output layer.
        \item computational graph mở dọc thời gian, tất cả bước $t$ dùng chung các trọng số và bias.
    \end{itemize}
\end{frame}

\begin{frame}{Phương trình forward và loss}
    \begin{align}
        \bm{h}_t &= \bm{W}_{hx} \bm{x}_t + \bm{W}_{hh} \bm{h}_{t-1} + \bm{b}_h, \tag{1} \\
        \bm{o}_t &= \bm{W}_{qh} \bm{h}_t + \bm{b}_q \tag{2}
    \end{align}
    \vspace{-0.5em}
    \begin{equation}
        \mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \ell\!\left(\bm{o}_t, \bm{y}_t\right) \tag{3}
    \end{equation}
    \begin{itemize}
        \item Hidden state nhận thông tin từ input hiện tại và hidden state trước đó.
        \item Loss trung bình giúp gradient không phụ thuộc độ dài chuỗi.
    \end{itemize}
\end{frame}

\begin{frame}{Đồ thị phụ thuộc}
    Để trực quan hóa phụ thuộc giữa biến và tham số trong quá trình tính toán RNN, ta vẽ một computational graph như Hình 9.7.2. Ví dụ, việc tính hidden state tại bước thời gian 3, $\bm{h}_3$, phụ thuộc vào các tham số $\bm{W}_{hx}$ và $\bm{W}_{hh}$, hidden state của bước trước $\bm{h}_2$, và input tại bước hiện tại $\bm{x}_3$.
    \begin{center}
        \includegraphics[width=\linewidth]{image.png}
    \end{center}
\end{frame}

\begin{frame}{Đồ thị phụ thuộc (tiếp)}
    Theo các phụ thuộc trong Hình 9.7.2, ta có thể đi ngược chiều mũi tên để lần lượt tính và lưu các gradient.
    \begin{center}
        \includegraphics[width=\linewidth]{image.png}
    \end{center}
\end{frame}

\begin{frame}{Gradient theo output tại time step $t$}

\begin{equation}
L = \frac{1}{T}\sum_{k=1}^{T}\ell(o_k,y_k).
\end{equation}

Vì $o_t$ chỉ xuất hiện trong một hạng tử của tổng, ta có:
\begin{align}
\frac{\partial L}{\partial o_t}
&=
\frac{\partial}{\partial o_t}\left(
\frac{1}{T}\sum_{k=1}^{T}\ell(o_k,y_k)
\right) \notag\\
&=
\frac{1}{T}\frac{\partial \ell(o_t,y_t)}{\partial o_t}
\in \mathbb{R}^{q\times 1}.
\end{align}

\end{frame}

\begin{frame}{Gradient theo $W_{qh}$ (1/2)}

Output layer tuyến tính:
\begin{equation}
o_t = W_{qh}h_t + b_q.
\end{equation}

Viết theo từng phần tử:
\begin{equation}
(o_t)_j = \sum_{i=1}^{h}(W_{qh})_{ji}(h_t)_i + (b_q)_j.
\end{equation}

Áp dụng chain rule:
\begin{align}
\frac{\partial L}{\partial (W_{qh})_{ji}}
&=
\sum_{t=1}^{T}\sum_{k=1}^{q}
\frac{\partial L}{\partial (o_t)_k}
\frac{\partial (o_t)_k}{\partial (W_{qh})_{ji}} \notag\\
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial (o_t)_j}(h_t)_i.
\end{align}

\end{frame}

\begin{frame}{Gradient theo $W_{qh}$ (2/2)}

Viết dưới dạng ma trận:
\begin{equation}
\frac{\partial L}{\partial W_{qh}}
=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial o_t}\right) h_t^\top
\in \mathbb{R}^{q\times h}.
\end{equation}

\begin{itemize}
    \item Gradient cộng dồn theo thời gian vì $W_{qh}$ được chia sẻ cho mọi bước.
\end{itemize}

\end{frame}

% \begin{frame}{Gradient theo $W_{qh}$ và bias $b_q$ ở output layer (1/2)}
%     Giả sử output layer tuyến tính có bias:
%     \begin{align}
%         o_t = W_{qh}h_t + b_q,\quad
%         W_{qh}\in\mathbb{R}^{q\times h},\;
%         h_t\in\mathbb{R}^{h},\;
%         b_q\in\mathbb{R}^{q} \notag \\
%         \frac{\partial L}{\partial W_{qh}}
%         =
%         \sum_{t=1}^{T}
%         \frac{\partial L}{\partial o_t}\;
%         \frac{\partial o_t}{\partial W_{qh}}
%         \tag{2a} \\
%         \frac{\partial L}{\partial W_{qh}}
%         =
%         \sum_{t=1}^{T}
%         \left(\frac{\partial L}{\partial o_t}\right) h_t^{\top}
%         \in \mathbb{R}^{q\times h}
%         \tag{2b}
%     \end{align}
%     \begin{itemize}
%         \item $\frac{\partial L}{\partial o_t}\in\mathbb{R}^{q}$ và $h_t^\top\in\mathbb{R}^{1\times h}$ nên $\left(\frac{\partial L}{\partial o_t}\right)h_t^\top$ là tích ngoài (outer product) cho ra ma trận $q\times h$.
%         \item $W_{qh}$ ảnh hưởng tới $L$ thông qua mọi $o_t$ nên gradient phải cộng dồn theo thời gian (sum over time).
%     \end{itemize}
% \end{frame}

% \begin{frame}{Gradient theo $W_{qh}$ và bias $b_q$ ở output layer (2/2)}
%     Giả sử output layer tuyến tính có bias:
%     \begin{align}
%         \frac{\partial L}{\partial b_q}
%         =
%         \sum_{t=1}^{T}
%         \frac{\partial L}{\partial o_t}\;
%         \frac{\partial o_t}{\partial b_q}
%         \tag{3a} \\
%         \frac{\partial o_t}{\partial b_q}=I_q \notag \\
%         \frac{\partial L}{\partial b_q}
%         =
%         \sum_{t=1}^{T}
%         \frac{\partial L}{\partial o_t}
%         \in \mathbb{R}^{q}
%         \tag{3b}
%     \end{align}
%     \begin{itemize}
%         \item Vì $o_t = W_{qh}h_t + b_q$ nên $\frac{\partial o_t}{\partial b_q}=I_q$, do đó gradient theo bias là tổng các $\frac{\partial L}{\partial o_t}$.
%         \item Thay $\frac{\partial L}{\partial o_t}$ từ (1) vào (2b) và (3b) để tính gradient khi train.
%     \end{itemize}
% \end{frame}

\begin{frame}{Gradient theo $h_T$}

Tại thời điểm cuối:
\begin{equation}
o_T = W_{qh}h_T + b_q.
\end{equation}

Áp dụng chain rule theo chỉ số:
\begin{align}
\frac{\partial L}{\partial (h_T)_i}
&=
\sum_{j=1}^{q}
\frac{\partial L}{\partial (o_T)_j}
\frac{\partial (o_T)_j}{\partial (h_T)_i} \notag\\
&=
\sum_{j=1}^{q}
(W_{qh})_{ji}
\frac{\partial L}{\partial (o_T)_j}.
\end{align}

Viết dạng vector:
\begin{equation}
\frac{\partial L}{\partial h_T}
=
W_{qh}^\top
\frac{\partial L}{\partial o_T}
\in \mathbb{R}^{h\times 1}.
\end{equation}

\end{frame}

\begin{frame}{Gradient theo $h_t$ với $t<T$}

Forward:
\begin{equation}
h_{t+1} = W_{hx}x_{t+1} + W_{hh}h_t + b_h,
\qquad
o_t = W_{qh}h_t + b_q.
\end{equation}

Áp dụng chain rule:
\begin{equation}
\frac{\partial L}{\partial h_t}
=
\left(\frac{\partial h_{t+1}}{\partial h_t}\right)^\top
\frac{\partial L}{\partial h_{t+1}}
+
\left(\frac{\partial o_t}{\partial h_t}\right)^\top
\frac{\partial L}{\partial o_t}.
\end{equation}

Vì các ánh xạ là tuyến tính:
\begin{equation}
\frac{\partial h_{t+1}}{\partial h_t} = W_{hh},
\qquad
\frac{\partial o_t}{\partial h_t} = W_{qh}.
\end{equation}

Suy ra:
\begin{equation}
\frac{\partial L}{\partial h_t}
=
W_{hh}^\top
\frac{\partial L}{\partial h_{t+1}}
+
W_{qh}^\top
\frac{\partial L}{\partial o_t}.
\end{equation}

\end{frame}

\begin{frame}{Gradient theo $W_{hx}$ (1/2)}

Hidden state tại thời điểm $t$:
\begin{equation}
h_t = W_{hx}x_t + W_{hh}h_{t-1} + b_h.
\end{equation}

Vì $L$ phụ thuộc vào $W_{hx}$ thông qua toàn bộ các $h_t$, ta áp dụng chain rule:
\begin{equation}
\frac{\partial L}{\partial W_{hx}}
=
\sum_{t=1}^{T}
\operatorname{prod}\!\left(
\frac{\partial L}{\partial h_t},
\frac{\partial h_t}{\partial W_{hx}}
\right).
\end{equation}

Xét từng phần tử của $h_t$:
\begin{equation}
(h_t)_i = \sum_{j=1}^{d}(W_{hx})_{ij}(x_t)_j + \sum_{k=1}^{h}(W_{hh})_{ik}(h_{t-1})_k + (b_h)_i.
\end{equation}

Suy ra:
\begin{equation}
\frac{\partial (h_t)_i}{\partial (W_{hx})_{ij}} = (x_t)_j.
\end{equation}

\end{frame}

\begin{frame}{Gradient theo $W_{hx}$ (2/2)}

Do đó:
\begin{align}
\frac{\partial L}{\partial (W_{hx})_{ij}}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial (h_t)_i}
\frac{\partial (h_t)_i}{\partial (W_{hx})_{ij}} \notag\\
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial (h_t)_i}(x_t)_j.
\end{align}

Viết dưới dạng ma trận:
\begin{equation}
\frac{\partial L}{\partial W_{hx}}
=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial h_t}\right) x_t^\top
\in \mathbb{R}^{h\times d}.
\end{equation}

% Viết dưới dạng ma trận:
% \begin{equation}
% \frac{\partial L}{\partial W_{hx}}
% =
% \sum_{t=1}^{T}
% \left(\frac{\partial L}{\partial h_t}\right) x_t^\top
% \in \mathbb{R}^{h\times d}.
% \end{equation}

\end{frame}

\begin{frame}{Gradient theo $W_{hh}$ (1/2)}

Từ phương trình hidden state:
\begin{equation}
h_t = W_{hx}x_t + W_{hh}h_{t-1} + b_h,
\end{equation}

$W_{hh}$ ảnh hưởng đến $L$ thông qua toàn bộ chuỗi hidden states $h_1,\ldots,h_T$.

Áp dụng chain rule:
\begin{equation}
\frac{\partial L}{\partial W_{hh}}
=
\sum_{t=1}^{T}
\operatorname{prod}\!\left(
\frac{\partial L}{\partial h_t},
\frac{\partial h_t}{\partial W_{hh}}
\right).
\end{equation}

Viết theo từng phần tử:
\begin{equation}
(h_t)_i = \sum_{k=1}^{h}(W_{hh})_{ik}(h_{t-1})_k + \cdots
\end{equation}

Suy ra:
\begin{equation}
\frac{\partial (h_t)_i}{\partial (W_{hh})_{ik}} = (h_{t-1})_k.
\end{equation}

\end{frame}

\begin{frame}{Gradient theo $W_{hh}$ (2/2)}

Do đó:
\begin{align}
\frac{\partial L}{\partial (W_{hh})_{ik}}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial (h_t)_i}
\frac{\partial (h_t)_i}{\partial (W_{hh})_{ik}} \notag\\
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial (h_t)_i}(h_{t-1})_k.
\end{align}

Viết dưới dạng ma trận:
\begin{equation}
\frac{\partial L}{\partial W_{hh}}
=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial h_t}\right) h_{t-1}^\top
\in \mathbb{R}^{h\times h}.
\end{equation}

\end{frame}

\begin{frame}{Gradient theo bias $b_h$}

Hidden state:
\begin{equation}
h_t = W_{hx}x_t + W_{hh}h_{t-1} + b_h.
\end{equation}

Xét đạo hàm theo $b_h$:
\begin{equation}
\frac{\partial h_t}{\partial b_h} = I_h.
\end{equation}

Áp dụng chain rule:
\begin{align}
\frac{\partial L}{\partial b_h}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial h_t}
\frac{\partial h_t}{\partial b_h} \notag\\
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial h_t}
\in \mathbb{R}^{h\times 1}.
\end{align}

\end{frame}

\begin{frame}{Gradient theo $b_q$}

Từ biểu thức:
\begin{equation}
o_t = W_{qh}h_t + b_q,
\end{equation}

ta có:
\begin{equation}
\frac{\partial o_t}{\partial b_q} = I_q.
\end{equation}

Áp dụng chain rule:
\begin{align}
\frac{\partial L}{\partial b_q}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial o_t}
\frac{\partial o_t}{\partial b_q} \notag\\
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial o_t}
\in \mathbb{R}^{q\times 1}.
\end{align}

\end{frame}

% \begin{frame}{Kết luận}

% Các biểu thức gradient trung tâm trong BPTT bao gồm:

% \medskip

% Gradient theo hidden state:
% \begin{equation}
% \frac{\partial L}{\partial h_t}
% =
% W_{hh}^\top
% \frac{\partial L}{\partial h_{t+1}}
% +
% W_{qh}^\top
% \frac{\partial L}{\partial o_t}.
% \end{equation}

% \medskip

% Gradient theo các tham số của mô hình:
% \begin{align}
% \frac{\partial L}{\partial W_{qh}}
% &=
% \sum_{t=1}^{T}
% \left(\frac{\partial L}{\partial o_t}\right) h_t^\top, \notag\\
% \frac{\partial L}{\partial W_{hx}}
% &=
% \sum_{t=1}^{T}
% \left(\frac{\partial L}{\partial h_t}\right) x_t^\top, \notag\\
% \frac{\partial L}{\partial W_{hh}}
% &=
% \sum_{t=1}^{T}
% \left(\frac{\partial L}{\partial h_t}\right) h_{t-1}^\top.
% \end{align}

% \end{frame}

\begin{frame}{Kết luận: Gradient theo các tham số của mô hình}
Gradient theo output layer:
\begin{align}
\frac{\partial L}{\partial W_{qh}}
&=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial o_t}\right) h_t^\top, \notag\\
\frac{\partial L}{\partial b_q}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial o_t}.
\end{align}

\medskip

Gradient theo hidden layer:
\begin{align}
\frac{\partial L}{\partial W_{hx}}
&=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial h_t}\right) x_t^\top, \notag\\
\frac{\partial L}{\partial W_{hh}}
&=
\sum_{t=1}^{T}
\left(\frac{\partial L}{\partial h_t}\right) h_{t-1}^\top, \notag\\
\frac{\partial L}{\partial b_h}
&=
\sum_{t=1}^{T}
\frac{\partial L}{\partial h_t}.
\end{align}

\medskip

Các biểu thức trên cung cấp đầy đủ gradient cần thiết
để cập nhật toàn bộ tham số của mô hình RNN trong BPTT.

\end{frame}

\end{document}
